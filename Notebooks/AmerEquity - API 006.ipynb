{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab185de-318b-4901-9ce1-1aa9f841ca6e",
   "metadata": {},
   "source": [
    "# American Equity Generative AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e704dd-566a-4c48-9faa-f9c1d07186c8",
   "metadata": {},
   "source": [
    "## Program Load ✅\n",
    "* This should include all the packages to import into the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da619f-9339-46f2-bbc7-ea6884d93a11",
   "metadata": {},
   "source": [
    "### Import Packages ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76269295-317d-4eb3-a0de-70fd65c67a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import streamlit as st\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, numpy as np\n",
    "import csv, sqlite3, textwrap, atexit\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import re\n",
    "import json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb977f9-8c95-4aa1-9bf7-410f1fcfb930",
   "metadata": {},
   "source": [
    "### API Key for OpenAI ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5102cdb5-d9db-42e7-babc-b19880ad5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  api_key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd393192-ae19-4329-b9ae-fffd205c5348",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SQL Schema ✅\n",
    "* Need to comment better in markdown instead of inside block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79691f4-b47c-4988-b2d4-9c056aaa7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ae_sales.sqlite from the five CSVs, plus views, FTS, and account policy\n",
    "\n",
    "# --- Hardcoded paths ---\n",
    "DB_PATH = \"data/ae_sales.sqlite\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# --- Connection policy ---\n",
    "PERSIST_CONN = True      # keep one connection alive for the app session\n",
    "_GLOBAL_CON  = None\n",
    "\n",
    "def _open_conn():\n",
    "    con = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
    "    con.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "    return con\n",
    "\n",
    "def get_conn():\n",
    "    \"\"\"Return a live connection; auto-reopen if closed.\"\"\"\n",
    "    global _GLOBAL_CON\n",
    "    try:\n",
    "        if _GLOBAL_CON is None:\n",
    "            _GLOBAL_CON = _open_conn()\n",
    "        else:\n",
    "            _GLOBAL_CON.execute(\"SELECT 1\")  # will raise if closed\n",
    "    except Exception:\n",
    "        _GLOBAL_CON = _open_conn()\n",
    "    return _GLOBAL_CON\n",
    "\n",
    "def get_cur():\n",
    "    return get_conn().cursor()\n",
    "\n",
    "def _close_conn():\n",
    "    global _GLOBAL_CON\n",
    "    try:\n",
    "        if _GLOBAL_CON is not None:\n",
    "            _GLOBAL_CON.close()\n",
    "    finally:\n",
    "        _GLOBAL_CON = None\n",
    "\n",
    "atexit.register(_close_conn)\n",
    "\n",
    "# --- Helpers ---\n",
    "def parse_date(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    if not s or s.lower() in {\"na\",\"n/a\",\"none\",\"null\"}:\n",
    "        return None\n",
    "    for fmt in (\"%m/%d/%y\", \"%m/%d/%Y\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt).date().isoformat()\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def to_int(s):\n",
    "    try:\n",
    "        return int(float(str(s).strip()))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def to_float(s):\n",
    "    try:\n",
    "        return float(str(s).strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def to_text(s):\n",
    "    return None if s is None else str(s)\n",
    "\n",
    "# === Bootstrap DB (build/refresh) ===\n",
    "con = get_conn()\n",
    "cur = get_cur()\n",
    "\n",
    "# Recreate for a clean start\n",
    "cur.executescript(\"\"\"\n",
    "DROP TABLE IF EXISTS sales_pipeline;\n",
    "DROP TABLE IF EXISTS sales_teams;\n",
    "DROP TABLE IF EXISTS products;\n",
    "DROP TABLE IF EXISTS accounts;\n",
    "DROP TABLE IF EXISTS interactions;\n",
    "DROP TABLE IF EXISTS interactions_fts;\n",
    "\"\"\")\n",
    "\n",
    "# --- Schema ---\n",
    "cur.executescript(\"\"\"\n",
    "CREATE TABLE accounts (\n",
    "    account_id         INTEGER PRIMARY KEY,\n",
    "    account            TEXT,\n",
    "    sector             TEXT,\n",
    "    year_established   INTEGER,\n",
    "    revenue            INTEGER,\n",
    "    employees          INTEGER,\n",
    "    office_location    TEXT,\n",
    "    subsidiary_of      TEXT,\n",
    "    propensity_to_buy  REAL\n",
    ");\n",
    "\n",
    "CREATE TABLE products (\n",
    "    product_id  INTEGER PRIMARY KEY,\n",
    "    product     TEXT,\n",
    "    series      TEXT,\n",
    "    sales_price INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE sales_teams (\n",
    "    sales_person_id INTEGER,\n",
    "    sales_agent     TEXT PRIMARY KEY,\n",
    "    manager         TEXT,\n",
    "    regional_office TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE sales_pipeline (\n",
    "    opportunity_id TEXT PRIMARY KEY,\n",
    "    account_id     INTEGER,\n",
    "    sales_agent    TEXT,\n",
    "    product_id     INTEGER,\n",
    "    product        TEXT,\n",
    "    account        TEXT,\n",
    "    deal_stage     TEXT,\n",
    "    engage_date    TEXT,   -- ISO date string\n",
    "    close_date     TEXT,   -- ISO date string\n",
    "    close_value    INTEGER,\n",
    "    FOREIGN KEY (account_id)  REFERENCES accounts(account_id),\n",
    "    FOREIGN KEY (sales_agent) REFERENCES sales_teams(sales_agent),\n",
    "    FOREIGN KEY (product_id)  REFERENCES products(product_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE interactions (\n",
    "    account_id     INTEGER,\n",
    "    account_name   TEXT,\n",
    "    contact_name   TEXT,\n",
    "    activity_type  TEXT,\n",
    "    status         TEXT,\n",
    "    timestamp      TEXT,   -- ISO date string\n",
    "    comment        TEXT\n",
    ");\n",
    "\n",
    "-- Helpful indexes\n",
    "CREATE INDEX IF NOT EXISTS idx_sp_account_id ON sales_pipeline(account_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_sp_product_id ON sales_pipeline(product_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_sp_deal_stage ON sales_pipeline(deal_stage);\n",
    "CREATE INDEX IF NOT EXISTS idx_interactions_account_id ON interactions(account_id);\n",
    "\"\"\")\n",
    "\n",
    "# --- Loaders (CSV -> typed rows -> executemany) ---\n",
    "def load_accounts():\n",
    "    path = DATA_DIR / \"accounts.csv\"\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        rows = [(\n",
    "            to_int(row.get(\"account_id\")),\n",
    "            to_text(row.get(\"account\")),\n",
    "            to_text(row.get(\"sector\")),\n",
    "            to_int(row.get(\"year_established\")),\n",
    "            to_int(row.get(\"revenue\")),\n",
    "            to_int(row.get(\"employees\")),\n",
    "            to_text(row.get(\"office_location\")),\n",
    "            to_text(row.get(\"subsidiary_of\")),\n",
    "            to_float(row.get(\"propensity_to_buy\")),\n",
    "        ) for row in r]\n",
    "    get_cur().executemany(\"\"\"\n",
    "        INSERT INTO accounts(\n",
    "            account_id, account, sector, year_established, revenue,\n",
    "            employees, office_location, subsidiary_of, propensity_to_buy\n",
    "        ) VALUES (?,?,?,?,?,?,?,?,?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "def load_products():\n",
    "    path = DATA_DIR / \"products.csv\"\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        rows = [(\n",
    "            to_int(row.get(\"product_id\")),\n",
    "            to_text(row.get(\"product\")),\n",
    "            to_text(row.get(\"series\")),\n",
    "            to_int(row.get(\"sales_price\")),\n",
    "        ) for row in r]\n",
    "    get_cur().executemany(\"\"\"\n",
    "        INSERT INTO products(product_id, product, series, sales_price)\n",
    "        VALUES (?,?,?,?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "def load_sales_teams():\n",
    "    path = DATA_DIR / \"sales_teams.csv\"\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        rows = [(\n",
    "            to_int(row.get(\"sales_person_id\")),\n",
    "            to_text(row.get(\"sales_agent\")),\n",
    "            to_text(row.get(\"manager\")),\n",
    "            to_text(row.get(\"regional_office\")),\n",
    "        ) for row in r]\n",
    "    get_cur().executemany(\"\"\"\n",
    "        INSERT INTO sales_teams(sales_person_id, sales_agent, manager, regional_office)\n",
    "        VALUES (?,?,?,?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "def load_sales_pipeline():\n",
    "    path = DATA_DIR / \"sales_pipeline.csv\"\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        rows = [(\n",
    "            to_text(row.get(\"opportunity_id\")),\n",
    "            to_int(row.get(\"account_id\")),\n",
    "            to_text(row.get(\"sales_agent\")),\n",
    "            to_int(row.get(\"product_id\")),\n",
    "            to_text(row.get(\"product\")),\n",
    "            to_text(row.get(\"account\")),\n",
    "            to_text(row.get(\"deal_stage\")),\n",
    "            parse_date(row.get(\"engage_date\")),\n",
    "            parse_date(row.get(\"close_date\")),\n",
    "            to_int(row.get(\"close_value\")),\n",
    "        ) for row in r]\n",
    "    get_cur().executemany(\"\"\"\n",
    "        INSERT INTO sales_pipeline(\n",
    "            opportunity_id, account_id, sales_agent, product_id, product, account,\n",
    "            deal_stage, engage_date, close_date, close_value\n",
    "        ) VALUES (?,?,?,?,?,?,?,?,?,?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "def load_interactions():\n",
    "    path = DATA_DIR / \"interactions.csv\"\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        rows = [(\n",
    "            to_int(row.get(\"account_id\")),\n",
    "            to_text(row.get(\"account_name\")),\n",
    "            to_text(row.get(\"contact_name\")),\n",
    "            to_text(row.get(\"activity_type\")),\n",
    "            to_text(row.get(\"status\")),\n",
    "            parse_date(row.get(\"timestamp\")),\n",
    "            to_text(row.get(\"comment\")),\n",
    "        ) for row in r]\n",
    "    get_cur().executemany(\"\"\"\n",
    "        INSERT INTO interactions(\n",
    "            account_id, account_name, contact_name, activity_type, status, timestamp, comment\n",
    "        ) VALUES (?,?,?,?,?,?,?)\n",
    "    \"\"\", rows)\n",
    "\n",
    "# --- Execute loads in FK-friendly order ---\n",
    "load_accounts()\n",
    "load_products()\n",
    "load_sales_teams()\n",
    "load_sales_pipeline()\n",
    "load_interactions()\n",
    "\n",
    "# --- Views and FTS for fast retrieval ---\n",
    "get_cur().executescript(\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS v_pipeline_open AS\n",
    "SELECT *\n",
    "FROM sales_pipeline\n",
    "WHERE deal_stage IN ('Prospecting','Engaging','Open','Negotiating');\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS v_bookings_month AS\n",
    "SELECT strftime('%Y-%m', close_date) AS yyyymm,\n",
    "       SUM(close_value) AS bookings\n",
    "FROM sales_pipeline\n",
    "WHERE deal_stage = 'Closed Won' AND close_date IS NOT NULL\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS interactions_fts USING fts5(\n",
    "    comment, account_id, activity_type, status, tokenize='porter'\n",
    ");\n",
    "\n",
    "INSERT INTO interactions_fts(rowid, comment, account_id, activity_type, status)\n",
    "SELECT rowid, comment, account_id, activity_type, status\n",
    "FROM interactions;\n",
    "\"\"\")\n",
    "\n",
    "# --- Account placeholder policy: backfill and triggers ---\n",
    "get_cur().executescript(\"\"\"\n",
    "INSERT INTO accounts (account_id, account, sector, year_established, revenue, employees, office_location, subsidiary_of, propensity_to_buy)\n",
    "SELECT 0, 'NO_ACCOUNT', 'Unknown', NULL, NULL, NULL, NULL, NULL, NULL\n",
    "WHERE NOT EXISTS (SELECT 1 FROM accounts WHERE account_id = 0);\n",
    "\n",
    "UPDATE sales_pipeline\n",
    "SET account_id = 0,\n",
    "    account    = 'NO_ACCOUNT'\n",
    "WHERE account_id IS NULL\n",
    "  AND deal_stage IN ('Engaging','Prospecting');\n",
    "\n",
    "UPDATE sales_pipeline\n",
    "SET account = (SELECT a.account FROM accounts a WHERE a.account_id = sales_pipeline.account_id)\n",
    "WHERE account IS NULL AND account_id IS NOT NULL;\n",
    "\n",
    "DROP TRIGGER IF EXISTS sp_ai_account_policy;\n",
    "CREATE TRIGGER sp_ai_account_policy\n",
    "AFTER INSERT ON sales_pipeline\n",
    "FOR EACH ROW\n",
    "BEGIN\n",
    "  UPDATE sales_pipeline\n",
    "     SET account_id = 0,\n",
    "         account    = 'NO_ACCOUNT'\n",
    "   WHERE opportunity_id = NEW.opportunity_id\n",
    "     AND NEW.deal_stage IN ('Engaging','Prospecting')\n",
    "     AND NEW.account_id IS NULL;\n",
    "\n",
    "  UPDATE sales_pipeline\n",
    "     SET account = (SELECT a.account FROM accounts a WHERE a.account_id = NEW.account_id)\n",
    "   WHERE opportunity_id = NEW.opportunity_id\n",
    "     AND NEW.account IS NULL\n",
    "     AND NEW.account_id IS NOT NULL;\n",
    "END;\n",
    "\n",
    "DROP TRIGGER IF EXISTS sp_au_account_policy;\n",
    "CREATE TRIGGER sp_au_account_policy\n",
    "AFTER UPDATE ON sales_pipeline\n",
    "FOR EACH ROW\n",
    "BEGIN\n",
    "  UPDATE sales_pipeline\n",
    "     SET account_id = 0,\n",
    "         account    = 'NO_ACCOUNT'\n",
    "   WHERE opportunity_id = NEW.opportunity_id\n",
    "     AND NEW.deal_stage IN ('Engaging','Prospecting')\n",
    "     AND NEW.account_id IS NULL;\n",
    "\n",
    "  UPDATE sales_pipeline\n",
    "     SET account = (SELECT a.account FROM accounts a WHERE a.account_id = NEW.account_id)\n",
    "   WHERE opportunity_id = NEW.opportunity_id\n",
    "     AND NEW.account IS NULL\n",
    "     AND NEW.account_id IS NOT NULL;\n",
    "END;\n",
    "\"\"\")\n",
    "\n",
    "# --- Quick summary check (optional) ---\n",
    "summary = dict(get_cur().execute(\"\"\"\n",
    "SELECT 'sp_null_account_id',   SUM(account_id IS NULL)\n",
    "FROM sales_pipeline\n",
    "UNION ALL\n",
    "SELECT 'sp_null_account_text', SUM(account IS NULL)\n",
    "FROM sales_pipeline\n",
    "\"\"\").fetchall())\n",
    "\n",
    "# Persist or close per policy\n",
    "if PERSIST_CONN:\n",
    "    # leave _GLOBAL_CON open for the rest of the app (sql_context, cache warming, etc.)\n",
    "    pass\n",
    "else:\n",
    "    _close_conn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3453d-aa45-47e0-a4c3-0ec3394d5ef2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load JSON Catalog ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4f8005-e659-43cc-8e84-83dcb611fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_DIR = Path(\"SQL\")\n",
    "SQL_CATALOG_PATH = Path(\"data/sql_catalog.json\")\n",
    "\n",
    "# in-memory global cache\n",
    "sql_catalog = None\n",
    "\n",
    "def load_sql_catalog(force_reload=False):\n",
    "    \"\"\"\n",
    "    Load the SQL catalog from JSON, restoring numpy embeddings.\n",
    "    \n",
    "    force_reload=True will ignore anything cached in memory.\n",
    "    \"\"\"\n",
    "    global sql_catalog\n",
    "\n",
    "    # If cached and not forced, return current version\n",
    "    if sql_catalog is not None and not force_reload:\n",
    "        return sql_catalog\n",
    "\n",
    "    # Load from JSON on disk\n",
    "    if SQL_CATALOG_PATH.exists():\n",
    "        raw = json.loads(SQL_CATALOG_PATH.read_text())\n",
    "\n",
    "        # Restore embedding arrays\n",
    "        for item in raw:\n",
    "            emb = item.get(\"emb\")\n",
    "            if isinstance(emb, list):\n",
    "                item[\"emb\"] = np.array(emb, dtype=float)\n",
    "\n",
    "        sql_catalog = raw\n",
    "        return sql_catalog\n",
    "\n",
    "    # If the JSON doesn't exist, bootstrap a new one\n",
    "    sql_catalog = [\n",
    "        {\n",
    "            \"name\": p.stem,\n",
    "            \"path\": str(p),\n",
    "            \"description\": f\"TODO: describe what {p.name} does.\"\n",
    "        }\n",
    "        for p in sorted(SQL_DIR.glob(\"*.sql\"))\n",
    "    ]\n",
    "\n",
    "    # Save initial file\n",
    "    SQL_CATALOG_PATH.write_text(json.dumps(sql_catalog, indent=2))\n",
    "    return sql_catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d35670-cccf-4d11-92ee-b0fbff029be9",
   "metadata": {},
   "source": [
    "### Caching ✅\n",
    "* Lightweight Caches (Recent Turns, Q&A Cache)\n",
    "* High-Value Caches (Frequent or FAST Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b8690-ba88-4ddf-ac81-631eae9f19a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lightweight Caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d886f8-9328-453d-b1f4-b52c19c2624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recent_turns = deque(maxlen=8)    # short chat context window\n",
    "qa_cache     = deque(maxlen=500)  # semantic Q&A cache\n",
    "\n",
    "def my_embed(text: str) -> np.ndarray:\n",
    "    # drop-in placeholder; you can replace with your SentenceTransformer or OpenAI embeddings later\n",
    "    import hashlib\n",
    "    return np.array([int(hashlib.sha1(text.encode()).hexdigest(), 16) % 1_000_000], dtype=float)\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = a.astype(float); b = b.astype(float)\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) or 1.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def find_in_cache(user_text: str, threshold: float = 0.92, embed_fn=my_embed):\n",
    "    if not qa_cache:\n",
    "        return None\n",
    "    q_vec = embed_fn(user_text)\n",
    "    best  = max(qa_cache, key=lambda qa: cosine_sim(q_vec, qa[\"q_emb\"]))\n",
    "    sim   = cosine_sim(q_vec, best[\"q_emb\"])\n",
    "    return best[\"answer\"] if sim >= threshold else None\n",
    "\n",
    "def add_to_cache(user_text: str, answer: str, embed_fn=my_embed):\n",
    "    qa_cache.append({\n",
    "        \"q_text\": user_text,\n",
    "        \"q_emb\":  embed_fn(user_text),\n",
    "        \"answer\": answer,\n",
    "        \"ts\":     datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47475b-fb13-404d-9008-3b9d517d5bb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### High-Value Caches\n",
    "* These queries are popular or go-to queries that we can help prepopulate.\n",
    "* Are these similar to the FAST path?\n",
    "* Do we want to have this be a table in a csv file rather than hard coded? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36debc50-eb16-48fa-a579-39a07a8c6147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/mc11fygd587gxt6r059dmknh0000gn/T/ipykernel_33756/3250319419.py:27: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"ts\":     datetime.utcnow().isoformat(timespec=\"seconds\"),\n"
     ]
    }
   ],
   "source": [
    "def warm_cache_from_sqlite(limit: int = 50):\n",
    "    # Example: common pipeline questions -> prebuild snippets\n",
    "    rows = cur.execute(\"\"\"\n",
    "        SELECT yyyymm, bookings FROM v_bookings_month ORDER BY yyyymm DESC LIMIT ?\n",
    "    \"\"\", (limit,)).fetchall()\n",
    "    if rows:\n",
    "        summary = \"Recent bookings by month:\\n\" + \"\\n\".join(f\"- {y}:{v}\" for y, v in rows[:12])\n",
    "        add_to_cache(\"What are recent bookings by month?\", summary)\n",
    "\n",
    "    # Example: seed recent activity blurbs (use FTS if you like)\n",
    "    rows2 = cur.execute(\"\"\"\n",
    "        SELECT account_id, activity_type, status, substr(comment,1,160)\n",
    "        FROM interactions\n",
    "        ORDER BY date(timestamp) DESC, rowid DESC\n",
    "        LIMIT ?\n",
    "    \"\"\", (limit,)).fetchall()\n",
    "    if rows2:\n",
    "        blurb = \"Recent interactions (latest):\\n\" + \"\\n\".join(\n",
    "            f\"- acct {a} • {t} • {s} • {c or ''}\" for a,t,s,c in rows2[:15]\n",
    "        )\n",
    "        add_to_cache(\"What happened recently with interactions?\", blurb)\n",
    "\n",
    "# Call once on app start (or behind a small toggle)\n",
    "warm_cache_from_sqlite()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a9e2c-2026-4983-8016-6db0beeb6f7b",
   "metadata": {},
   "source": [
    "## Prescribed SQL Statements ✅\n",
    "* User Input Cleaned - This cleans the user input into a 25 or less word sentence to match the input to a SQL prompt. \n",
    "* Embeddings & Regressions\n",
    "* Load Canned SQL Statements Database\n",
    "* Run SQL Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67568513-b976-4515-949c-f0f6034199bb",
   "metadata": {},
   "source": [
    "### User Input Cleaned ✅\n",
    "* def clean_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73081058-d3f4-426b-a7f7-879dab9a8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(raw_question: str) -> str:\n",
    "    \"\"\"Rewrite a noisy question into a short, precise analytics request.\"\"\"\n",
    "    prompt = (\n",
    "        \"Rewrite the user's question as a short, clear analytics request \"\n",
    "        \"about the American Equity sales data. Keep it under 25 words.\\n\\n\"\n",
    "        f\"User question: {raw_question}\"\n",
    "    )\n",
    "    resp = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=prompt,\n",
    "    )\n",
    "    cleaned = resp.output_text.strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fd6a3-61a7-4637-ab06-44485b348610",
   "metadata": {},
   "source": [
    "### Embeddings & Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa541f-19aa-40d4-8ee5-6fc763100e72",
   "metadata": {},
   "source": [
    "#### Empties SQL Catalog Cache ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813428db-47b2-4788-a957-4502143034c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_DIR = Path(\"SQL\")\n",
    "SQL_CATALOG_PATH = Path(\"data/sql_catalog.json\")\n",
    "\n",
    "sql_catalog = []  # global cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a9c25-209e-4b24-9bb8-6b8dd04b1038",
   "metadata": {},
   "source": [
    "#### Loads Catalog ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb8f9df-7cd0-4da4-89cd-08154b27c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sql_catalog():\n",
    "    \"\"\"\n",
    "    Load the SQL catalog file, restoring embeddings as numpy arrays.\n",
    "    If it does not exist, bootstrap entries from filenames.\n",
    "    \"\"\"\n",
    "    global sql_catalog\n",
    "\n",
    "    # Load from disk\n",
    "    raw = json.loads(SQL_CATALOG_PATH.read_text())\n",
    "\n",
    "    # restore embeddings from lists → numpy arrays\n",
    "    for item in raw:\n",
    "        emb = item.get(\"emb\")\n",
    "        if isinstance(emb, list):\n",
    "            item[\"emb\"] = np.array(emb, dtype=float)\n",
    "\n",
    "    sql_catalog = raw\n",
    "    return sql_catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67f6ba-f604-474e-9d35-80db304b3c0c",
   "metadata": {},
   "source": [
    "#### Generates Vectors from User Text into Embedding Model ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325c319a-2094-4454-9067-bd8b16cf8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text: str) -> np.ndarray:\n",
    "    # Guardrail: make sure we are embedding a string\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(f\"embed_text expected a string, got {type(text)}\")\n",
    "\n",
    "    resp = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text,          # <-- string, not [text]\n",
    "    )\n",
    "    return np.array(resp.data[0].embedding, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b64502-bf98-4335-abbf-0b5819bb7736",
   "metadata": {},
   "source": [
    "#### If JSON is updated, this creates a new emb if there isn't an instance ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b4f570-bdc7-49d1-af0b-465505565a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "697373"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_catalog = load_sql_catalog()\n",
    "\n",
    "for item in sql_catalog:\n",
    "    if \"emb\" not in item or not isinstance(item[\"emb\"], np.ndarray):\n",
    "        item[\"emb\"] = embed_text(item[\"description\"])\n",
    "\n",
    "# Persist embeddings back to disk as plain lists\n",
    "_catalog_for_disk = deepcopy(sql_catalog)\n",
    "for item in _catalog_for_disk:\n",
    "    if isinstance(item.get(\"emb\"), np.ndarray):\n",
    "        item[\"emb\"] = item[\"emb\"].tolist()\n",
    "\n",
    "SQL_CATALOG_PATH.write_text(json.dumps(_catalog_for_disk, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c50b6-a313-4529-8ae9-791a8c866c00",
   "metadata": {},
   "source": [
    "#### Creates a Similarity Score for each SQL ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d91e7d-1af7-426c-be35-8df4aafa1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sql_queries(cleaned_question: str):\n",
    "    if not sql_catalog:\n",
    "        load_sql_catalog()\n",
    "\n",
    "    q_vec = embed_text(cleaned_question)\n",
    "    q_norm = q_vec / (np.linalg.norm(q_vec) or 1.0)\n",
    "\n",
    "    scored = []\n",
    "    for item in sql_catalog:\n",
    "        emb = np.array(item[\"emb\"], dtype=float)\n",
    "        emb_norm = emb / (np.linalg.norm(emb) or 1.0)\n",
    "        sim = float(np.dot(q_norm, emb_norm))\n",
    "        scored.append({\n",
    "            \"item\": item,\n",
    "            \"sim\": sim,\n",
    "        })\n",
    "\n",
    "    scored.sort(key=lambda d: d[\"sim\"], reverse=True)\n",
    "    return scored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b524a-3f03-4697-be72-09f77b5ef308",
   "metadata": {},
   "source": [
    "### Select Canned SQL Question ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a85225-c262-45ca-8ed1-9e3be3eacd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sql_for_question(raw_question: str):\n",
    "    cleaned = clean_question(raw_question)\n",
    "    ranked = rank_sql_queries(cleaned)\n",
    "\n",
    "    best = ranked[0]\n",
    "    second = ranked[1] if len(ranked) > 1 else None\n",
    "\n",
    "    sim_best = best[\"sim\"]\n",
    "    sim_second = second[\"sim\"] if second else 0.0\n",
    "    gap = sim_best - sim_second\n",
    "\n",
    "    # Suggested thresholds for similarity\n",
    "    if sim_best >= 0.45 and gap >= 0.04:\n",
    "        decision = \"high_confidence\"\n",
    "    elif sim_best >= 0.30:\n",
    "        decision = \"medium_confidence\"\n",
    "    else:\n",
    "        decision = \"low_confidence\"\n",
    "\n",
    "    return {\n",
    "        \"raw_question\": raw_question,\n",
    "        \"cleaned_question\": cleaned,\n",
    "        \"best\": best,\n",
    "        \"second\": second,\n",
    "        \"sim_best\": sim_best,\n",
    "        \"sim_second\": sim_second,\n",
    "        \"gap\": gap,\n",
    "        \"decision\": decision,\n",
    "        \"all\": ranked[:5]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bda6484-4f8a-412d-acc7-2b07b1aab862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_question': 'Which parts of our sales organization and product lineup appear to generate most of our recent closed business?',\n",
       " 'cleaned_question': 'Analyze American Equity sales data to identify which parts of the sales organization and which product lines generate the most recently closed business.',\n",
       " 'best': {'item': {'name': 'product_price_range',\n",
       "   'path': 'SQL/product_price_range.sql',\n",
       "   'description': 'Show minimum, maximum, and average closed-won deal values for each product.',\n",
       "   'emb': array([-0.01168524,  0.01348875,  0.00886725, ..., -0.01563041,\n",
       "           0.0259505 ,  0.01396467])},\n",
       "  'sim': 0.5011501039428546},\n",
       " 'second': {'item': {'name': 'sales_pipeline_in-progress',\n",
       "   'path': 'SQL/sales_pipeline_in-progress.sql',\n",
       "   'description': 'List all active sales opportunities with stages and expected close dates.',\n",
       "   'emb': array([-0.02232847,  0.01979916,  0.01882273, ...,  0.00361161,\n",
       "           0.00200139,  0.00301458])},\n",
       "  'sim': 0.47554058697666635},\n",
       " 'sim_best': 0.5011501039428546,\n",
       " 'sim_second': 0.47554058697666635,\n",
       " 'gap': 0.025609516966188295,\n",
       " 'decision': 'medium_confidence',\n",
       " 'all': [{'item': {'name': 'product_price_range',\n",
       "    'path': 'SQL/product_price_range.sql',\n",
       "    'description': 'Show minimum, maximum, and average closed-won deal values for each product.',\n",
       "    'emb': array([-0.01168524,  0.01348875,  0.00886725, ..., -0.01563041,\n",
       "            0.0259505 ,  0.01396467])},\n",
       "   'sim': 0.5011501039428546},\n",
       "  {'item': {'name': 'sales_pipeline_in-progress',\n",
       "    'path': 'SQL/sales_pipeline_in-progress.sql',\n",
       "    'description': 'List all active sales opportunities with stages and expected close dates.',\n",
       "    'emb': array([-0.02232847,  0.01979916,  0.01882273, ...,  0.00361161,\n",
       "            0.00200139,  0.00301458])},\n",
       "   'sim': 0.47554058697666635},\n",
       "  {'item': {'name': 'sales_team_stats',\n",
       "    'path': 'SQL/sales_team_stats.sql',\n",
       "    'description': 'Summarize sales teams by counting agents per manager and regional office.',\n",
       "    'emb': array([ 0.00146266,  0.03724608,  0.06618843, ..., -0.01721092,\n",
       "           -0.00143927,  0.0254181 ])},\n",
       "   'sim': 0.4579301462497082},\n",
       "  {'item': {'name': 'revenue_for_Q2',\n",
       "    'path': 'SQL/revenue_for_Q2.sql',\n",
       "    'description': 'Show total revenue and won deal counts for second-quarter sales.',\n",
       "    'emb': array([-0.00125031, -0.01309817, -0.01706956, ..., -0.02960039,\n",
       "            0.04570793,  0.02490132])},\n",
       "   'sim': 0.45291030520298575},\n",
       "  {'item': {'name': 'revenue_for_Q4',\n",
       "    'path': 'SQL/revenue_for_Q4sql.sql',\n",
       "    'description': 'Show total revenue and won deal counts for fourth-quarter sales.',\n",
       "    'emb': array([ 0.00185596, -0.01641906, -0.01998251, ..., -0.03365475,\n",
       "            0.04731461,  0.03056148])},\n",
       "   'sim': 0.45218733707606906}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_sql_for_question(\"Which parts of our sales organization and product lineup appear to generate most of our recent closed business?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef58d47-365f-4dca-b353-dcc828bfe2f2",
   "metadata": {},
   "source": [
    "### Routing logic ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea41466-e05b-4fda-8ca7-11ab3d6adb25",
   "metadata": {},
   "source": [
    "#### High confidence\n",
    "* sim_best >= 0.45 and gap to second is at least 0.01\n",
    "* Auto-run the matched SQL\n",
    "\n",
    "#### Medium confidence\n",
    "* sim_best >= 0.30\n",
    "* Suggest top match (and maybe second) to user, or still auto-run if you want to be aggressive\n",
    "\n",
    "#### Low confidence\n",
    "* sim_best < 0.30\n",
    "* Do not trust catalog, fall back to “generate new SELECT” with a bigger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a876d050-a9d4-4709-9041-b26b87f662e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(raw_question: str):\n",
    "    \"\"\"\n",
    "    Decide whether to:\n",
    "      - use a predefined SQL from the catalog\n",
    "      - or fall back to generating a new SELECT for the full dataset\n",
    "    \"\"\"\n",
    "    cleaned = clean_question(raw_question)\n",
    "    ranked = rank_sql_queries(cleaned)\n",
    "\n",
    "    best = ranked[0]\n",
    "    second = ranked[1] if len(ranked) > 1 else None\n",
    "\n",
    "    sim_best = best[\"sim\"]\n",
    "    sim_second = second[\"sim\"] if second else 0.0\n",
    "    gap = sim_best - sim_second\n",
    "\n",
    "    if sim_best >= 0.45 and gap >= 0.01:\n",
    "        mode = \"use_catalog_high_conf\"\n",
    "    elif sim_best >= 0.3:\n",
    "        mode = \"use_catalog_medium_conf\"\n",
    "    else:\n",
    "        mode = \"generate_new_sql\"\n",
    "\n",
    "    return {\n",
    "        \"raw_question\": raw_question,\n",
    "        \"cleaned_question\": cleaned,\n",
    "        \"mode\": mode,\n",
    "        \"best\": best,\n",
    "        \"second\": second,\n",
    "        \"sim_best\": sim_best,\n",
    "        \"sim_second\": sim_second,\n",
    "        \"gap\": gap,\n",
    "        \"all\": ranked[:5],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606b3d9-53b0-4207-b7f7-9512efb43e84",
   "metadata": {},
   "source": [
    "### Execute Top Ranked SQL File ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e039d1-7b97-4054-b3a4-63b28441f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_from_catalog_item(item) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute the SQL file pointed at by `item` and return a DataFrame.\n",
    "    \"\"\"\n",
    "    sql_path = Path(item[\"path\"])\n",
    "    sql_text = sql_path.read_text()\n",
    "\n",
    "    con = get_conn()     # your SQLite connection builder\n",
    "    df = pd.read_sql_query(sql_text, con)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6dfe96-0af2-44dc-8a05-ddcbb559396b",
   "metadata": {},
   "source": [
    "### Canned SQL Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b59d9-f3cd-456b-8f01-971d98c1d9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6062d22c-98bf-4168-b435-656c13b276ad",
   "metadata": {},
   "source": [
    "### Run SQL Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff053a-20a2-484d-a7f2-273c04eaba49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0af25800-172b-465f-9769-eac3dfbd7d3b",
   "metadata": {},
   "source": [
    "## Bundle Input for Query\n",
    "* Summarize Recent Turns - Collects the most recent user–assistant exchanges into a concise summary. This helps the model remember context from earlier in the chat without resending the entire conversation.\n",
    "* Maybe Lookup SQL - Runs lightweight, rule-based SQL queries that extract just the data most relevant to the current question. It anchors the model’s reasoning in actual database values instead of relying solely on text patterns.\n",
    "* Needs More Data - Applies heuristics to decide when the lightweight context isn’t enough (for example, the user requests detailed statistics or row-level data). It signals whether to escalate to a larger “full dataset” retrieval and the more capable model.\n",
    "* Build Full Data Context - Generates compact summaries from larger database slices—counts, aggregates, or top records—without overwhelming the token budget. This ensures that heavy analytical questions can still be answered efficiently and accurately.\n",
    "* Build Bundled Input - Combines everything—chat history, cached responses, SQL context, optional full data summaries, and the user’s question—into a single structured message for the LLM. This unified bundle gives the model the richest, most efficient context possible for consistent, data-grounded answers.\n",
    "* build_sql_commentary_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87ce04b5-9183-4e38-bb44-026e8d924609",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"fast\":  \"gpt-5-nano\",   # low latency, cheap\n",
    "    \"smart\": \"gpt-5\",  # higher quality for heavy queries\n",
    "}\n",
    "\n",
    "def summarize_recent_turns(n: int = 6) -> str:\n",
    "    if not recent_turns:\n",
    "        return \"\"\n",
    "    turns = list(recent_turns)[-n:]\n",
    "    lines = []\n",
    "    for role, content in turns:\n",
    "        lines.append(f\"{role.upper()}: {content.strip()}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def maybe_lookup_sql(user_text: str) -> str:\n",
    "    # Tiny rule-based mapper; expand later with your flow chart\n",
    "    text = user_text.lower()\n",
    "    ctx  = []\n",
    "    if \"booking\" in text or \"revenue\" in text:\n",
    "        ctx.append(\"/* SQL: monthly bookings */\")\n",
    "        rows = cur.execute(\"\"\"\n",
    "            SELECT yyyymm, bookings\n",
    "            FROM v_bookings_month\n",
    "            ORDER BY yyyymm DESC LIMIT 6\n",
    "        \"\"\").fetchall()\n",
    "        if rows:\n",
    "            ctx.append(\"\\n\".join(f\"{y}: {b}\" for y,b in rows))\n",
    "    if \"interaction\" in text or \"activity\" in text:\n",
    "        ctx.append(\"/* SQL: last interactions */\")\n",
    "        rows = cur.execute(\"\"\"\n",
    "            SELECT account_id, activity_type, status, substr(comment,1,120)\n",
    "            FROM interactions\n",
    "            ORDER BY date(timestamp) DESC, rowid DESC\n",
    "            LIMIT 10\n",
    "        \"\"\").fetchall()\n",
    "        for a,t,s,c in rows:\n",
    "            ctx.append(f\"acct {a} • {t} • {s} • {c or ''}\")\n",
    "    return \"\\n\".join(ctx)\n",
    "\n",
    "def needs_more_data(user_text: str, sql_ctx: str, cached_ans: str | None) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: return True when the light bundle is likely insufficient.\n",
    "    Tune these rules to your flow chart.\n",
    "    \"\"\"\n",
    "    t = user_text.lower()\n",
    "    wants_row_level = any(k in t for k in [\n",
    "        \"all rows\", \"full dataset\", \"entire dataset\", \"export\", \"everything\",\n",
    "        \"per account breakdown\", \"per rep breakdown\", \"line item\", \"detail view\"\n",
    "    ])\n",
    "    wants_stats = any(k in t for k in [\n",
    "        \"distribution\", \"histogram\", \"outlier\", \"correlation\", \"anova\",\n",
    "        \"regression\", \"forecast\", \"time series\", \"per product over time\"\n",
    "    ])\n",
    "    no_sql_found = not sql_ctx or len(sql_ctx.strip()) < 40\n",
    "    no_cache = cached_ans is None\n",
    "\n",
    "    # Trigger if they ask for heavy tasks, or our limited context is empty, or both\n",
    "    return wants_row_level or wants_stats or (no_sql_found and no_cache)\n",
    "\n",
    "\n",
    "def build_full_data_context(limit_per_table: int = 2000) -> str:\n",
    "    \"\"\"\n",
    "    Load larger slices for deeper analysis. We still should not dump millions of tokens.\n",
    "    Instead, compute compact summaries. Adjust limits to your needs.\n",
    "    \"\"\"\n",
    "    cur = get_cur()\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # 1) High level counts\n",
    "    counts = {}\n",
    "    for t in [\"accounts\",\"products\",\"sales_teams\",\"sales_pipeline\",\"interactions\"]:\n",
    "        counts[t] = cur.execute(f\"SELECT COUNT(*) FROM {t}\").fetchone()[0]\n",
    "    parts.append(\"[full:row_counts]\\n\" + \", \".join(f\"{k}={v}\" for k,v in counts.items()))\n",
    "\n",
    "    # 2) Pipeline stats by stage\n",
    "    rows = cur.execute(\"\"\"\n",
    "        SELECT deal_stage, COUNT(*) AS n, SUM(COALESCE(close_value,0)) AS value_sum\n",
    "        FROM sales_pipeline\n",
    "        GROUP BY 1 ORDER BY n DESC\n",
    "    \"\"\").fetchall()\n",
    "    if rows:\n",
    "        parts.append(\"[full:pipeline_by_stage]\\n\" + \"\\n\".join(f\"{s}: n={n}, sum={v}\" for s,n,v in rows))\n",
    "\n",
    "    # 3) Bookings by month full history\n",
    "    rows = cur.execute(\"\"\"\n",
    "        SELECT yyyymm, bookings FROM v_bookings_month ORDER BY yyyymm\n",
    "    \"\"\").fetchall()\n",
    "    if rows:\n",
    "        parts.append(\"[full:bookings_month_all]\\n\" + \"\\n\".join(f\"{y}: {b}\" for y,b in rows))\n",
    "\n",
    "    # 4) Recent interactions bigger window\n",
    "    rows = cur.execute(\"\"\"\n",
    "        SELECT account_id, activity_type, status, substr(comment,1,160), timestamp\n",
    "        FROM interactions\n",
    "        ORDER BY date(timestamp) DESC, rowid DESC\n",
    "        LIMIT ?\n",
    "    \"\"\", (limit_per_table,)).fetchall()\n",
    "    if rows:\n",
    "        parts.append(\"[full:interactions_recent]\\n\" + \"\\n\".join(\n",
    "            f\"acct {a} • {t} • {s} • {c or ''} • {ts or ''}\" for a,t,s,c,ts in rows\n",
    "        ))\n",
    "\n",
    "    # 5) Top accounts by value\n",
    "    rows = cur.execute(\"\"\"\n",
    "        SELECT COALESCE(account,'NO_ACCOUNT') AS account, SUM(COALESCE(close_value,0)) AS val\n",
    "        FROM sales_pipeline\n",
    "        WHERE deal_stage='Closed Won'\n",
    "        GROUP BY 1\n",
    "        ORDER BY val DESC\n",
    "        LIMIT 50\n",
    "    \"\"\").fetchall()\n",
    "    if rows:\n",
    "        parts.append(\"[full:top_accounts_won]\\n\" + \"\\n\".join(f\"{a}: {v}\" for a,v in rows))\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "def build_bundled_input(user_text: str, use_full: bool = False) -> str:\n",
    "    chat_ctx   = summarize_recent_turns()\n",
    "    cached_ans = find_in_cache(user_text)[0] if 'find_in_cache' in globals() else None\n",
    "    sql_ctx    = maybe_lookup_sql(user_text)\n",
    "    full_ctx   = build_full_data_context() if use_full else \"\"\n",
    "\n",
    "    sections = []\n",
    "    if chat_ctx:\n",
    "        sections.append(\"### Recent chat context\\n\" + chat_ctx)\n",
    "    if cached_ans:\n",
    "        sections.append(\"### Previously answered (for reference)\\n\" + cached_ans)\n",
    "    if sql_ctx:\n",
    "        sections.append(\"### Data context (SQLite views)\\n\" + sql_ctx)\n",
    "    if full_ctx:\n",
    "        sections.append(\"### Expanded data context (full dataset summaries)\\n\" + full_ctx)\n",
    "\n",
    "    sections.append(\"### Question\\n\" + user_text)\n",
    "    return \"\\n\\n\".join(sections)\n",
    "\n",
    "\n",
    "def build_sql_commentary_prompt(user_question: str,\n",
    "                                sql_item: dict,\n",
    "                                df: pd.DataFrame,\n",
    "                                max_rows: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt that asks the LLM to explain what this canned SQL\n",
    "    and its results mean for a sales user.\n",
    "    \"\"\"\n",
    "\n",
    "    query_name = sql_item.get(\"name\", \"Unnamed query\")\n",
    "    query_desc = sql_item.get(\"description\", \"\").strip()\n",
    "\n",
    "    sample = df.head(max_rows) if df is not None else None\n",
    "\n",
    "    if sample is not None and not sample.empty:\n",
    "        sample_markdown = sample.to_markdown(index=False)\n",
    "        cols = \", \".join(sample.columns)\n",
    "    else:\n",
    "        sample_markdown = \"No rows returned.\"\n",
    "        cols = \"no columns\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a sales analytics assistant for American Equity.\n",
    "\n",
    "The user asked this question:\n",
    "{user_question}\n",
    "\n",
    "We matched their question to this canned SQL query:\n",
    "\n",
    "Name: {query_name}\n",
    "Description: {query_desc}\n",
    "\n",
    "The query returned a table with columns:\n",
    "{cols}\n",
    "\n",
    "Here is a small sample of the results:\n",
    "{sample_markdown}\n",
    "\n",
    "Write a short explanation in plain business language for a sales person. Please:\n",
    "- Explain what this query is doing and what the table represents.\n",
    "- Call out any key numbers or patterns that stand out.\n",
    "- Give 2 or 3 concrete insights or talking points they could share with their manager or a wholesaler.\n",
    "- If the table is empty or very small, explain what that might mean.\n",
    "\n",
    "Keep it concise, conversational, and non technical.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def llm_summarize(prompt: str) -> str:\n",
    "    \"\"\"Send a prompt to GPT-5 to generate business-friendly commentary.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        input=prompt\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "def ensure_qa_history_table(conn):\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS qa_history (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            ts TEXT,\n",
    "            question TEXT,\n",
    "            answer TEXT,\n",
    "            route_mode TEXT,\n",
    "            sql_item_name TEXT,\n",
    "            sql_item_description TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "def save_qa(conn, question, answer, route_mode, sql_item=None):\n",
    "    ensure_qa_history_table(conn)\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO qa_history (ts, question, answer, route_mode, sql_item_name, sql_item_description)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (\n",
    "            datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "            question,\n",
    "            answer,\n",
    "            route_mode,\n",
    "            sql_item.get(\"name\") if sql_item else None,\n",
    "            sql_item.get(\"description\") if sql_item else None,\n",
    "        ),\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "def get_past_answers_for_sql(conn, sql_item_name: str, limit: int = 5):\n",
    "    \"\"\"Return recent past Q&A rows that used the same canned SQL.\"\"\"\n",
    "    ensure_qa_history_table(conn)\n",
    "    cur = conn.execute(\n",
    "        \"\"\"\n",
    "        SELECT ts, question, answer\n",
    "        FROM qa_history\n",
    "        WHERE sql_item_name = ?\n",
    "        ORDER BY ts DESC\n",
    "        LIMIT ?\n",
    "        \"\"\",\n",
    "        (sql_item_name, limit),\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    return [\n",
    "        {\"ts\": r[0], \"question\": r[1], \"answer\": r[2]}\n",
    "        for r in rows\n",
    "    ]\n",
    "\n",
    "def build_historical_comparison_prompt(\n",
    "    user_question: str,\n",
    "    sql_item: dict,\n",
    "    current_df: pd.DataFrame | None,\n",
    "    current_commentary: str | None,\n",
    "    past_qas: list,\n",
    "    max_rows: int = 10,\n",
    ") -> str:\n",
    "\n",
    "    sample = current_df.head(max_rows) if current_df is not None else None\n",
    "    if sample is not None and not sample.empty:\n",
    "        sample_markdown = sample.to_markdown(index=False)\n",
    "    else:\n",
    "        sample_markdown = \"No rows returned.\"\n",
    "\n",
    "    history_block = \"\"\n",
    "    for qa in past_qas:\n",
    "        history_block += f\"- [{qa['ts']}] Question: {qa['question']}\\n\"\n",
    "        history_block += f\"  Answer snapshot:\\n{qa['answer']}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a sales analytics assistant for American Equity.\n",
    "\n",
    "The user just asked:\n",
    "{user_question}\n",
    "\n",
    "This was matched to canned SQL:\n",
    "{sql_item.get('name')} - {sql_item.get('description', '').strip()}\n",
    "\n",
    "Current result sample:\n",
    "{sample_markdown}\n",
    "\n",
    "Current commentary:\n",
    "{current_commentary or '(no commentary yet)'}\n",
    "\n",
    "Here are previous related questions and answers that used this same SQL:\n",
    "{history_block or '(no prior related answers found.)'}\n",
    "\n",
    "Please:\n",
    "1. Compare the current result to the earlier answers. If numbers seem higher, lower, or different in structure, call that out.\n",
    "2. Give 2 or 3 concise insights that place the current result in context of history.\n",
    "3. Suggest one follow up question or analysis the user might ask next.\n",
    "\n",
    "Keep it short and focused on business interpretation.\n",
    "\"\"\"\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349e47f-2907-4b15-beb5-08bc5564f29e",
   "metadata": {},
   "source": [
    "## Router\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7624de38-2a10-442b-917c-d8c593497c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(user_text: str) -> str:\n",
    "    # A) Try cache first\n",
    "    cached = find_in_cache(user_text)\n",
    "    if cached:\n",
    "        return cached\n",
    "\n",
    "    # B) Build bundle and call model\n",
    "    bundle = build_bundled_input(user_text)\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL_ID,\n",
    "        input=bundle,\n",
    "        store=True,\n",
    "    )\n",
    "    answer = resp.output_text\n",
    "\n",
    "    # C) Record turn + cache the successful answer\n",
    "    recent_turns.append((\"user\", user_text))\n",
    "    recent_turns.append((\"assistant\", answer))\n",
    "    add_to_cache(user_text, answer)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc34624-a4fd-4068-aa20-8ec9c20eb34a",
   "metadata": {},
   "source": [
    "## App UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cd6e865-6e59-4250-aad4-ec2a711f2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Session state setup ------------------------------------------------------\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []  # each item: dict(role, type, content, [data])\n",
    "\n",
    "# Only show welcome once per browser session\n",
    "if \"has_shown_welcome\" not in st.session_state:\n",
    "    st.session_state.has_shown_welcome = False\n",
    "\n",
    "if not st.session_state.has_shown_welcome:\n",
    "    welcome_text = textwrap.dedent(\n",
    "        \"\"\"\n",
    "        Welcome to the American Equity Sales Assistant.\n",
    "\n",
    "        You can ask natural language questions and the app will:\n",
    "        - Try to match your question to a curated SQL query from the catalog\n",
    "        - Run that SQL against the warehouse\n",
    "        - Summarize the results for you\n",
    "\n",
    "        A few example questions:\n",
    "        - \"Can you give me a quick summary of activity and revenue by account?\"\n",
    "        - “Who are the customers most likely to buy from us?”\n",
    "        - \"Rank customers by a combination of total revenue and propensity to purchase.\"\n",
    "        - “Show me the biggest deals we’ve won.”\n",
    "\n",
    "        When you are ready, type your question in the chat box below.\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "\n",
    "    st.session_state.messages.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"type\": \"text\",\n",
    "            \"content\": welcome_text,\n",
    "        }\n",
    "    )\n",
    "    st.session_state.has_shown_welcome = True\n",
    "\n",
    "st.title(\"American Equity Assistant\")\n",
    "\n",
    "# --- 1. Render history --------------------------------------------------------\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        if msg.get(\"type\", \"text\") == \"text\":\n",
    "            st.markdown(msg[\"content\"])\n",
    "        elif msg[\"type\"] == \"table\":\n",
    "            st.markdown(msg[\"content\"])\n",
    "            df_hist = pd.DataFrame(msg[\"data\"])\n",
    "            st.dataframe(df_hist)\n",
    "\n",
    "# --- 2. New user input --------------------------------------------------------\n",
    "user_text = st.chat_input(\"Ask a question about the sales data...\")\n",
    "\n",
    "if user_text:\n",
    "    # Show user bubble for this turn\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_text)\n",
    "\n",
    "    # Save user message in history\n",
    "    st.session_state.messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"type\": \"text\",\n",
    "            \"content\": user_text,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    raw_question = user_text\n",
    "\n",
    "    # --- 3. Build lightweight context ----------------------------------------\n",
    "    chat_ctx = summarize_recent_turns()\n",
    "    cached = find_in_cache(user_text)\n",
    "    cached_ans = cached[0] if cached else None\n",
    "    sql_ctx = maybe_lookup_sql(user_text)\n",
    "\n",
    "    # --- 4. Route question ----------------------------------------------------\n",
    "    route = route_question(raw_question)\n",
    "\n",
    "    # High or medium confidence match to canned SQL\n",
    "    if route[\"mode\"] in (\"use_catalog_high_conf\", \"use_catalog_medium_conf\"):\n",
    "        sql_item = route[\"best\"][\"item\"]\n",
    "        df = run_sql_from_catalog_item(sql_item)\n",
    "\n",
    "        # grab past Q&A for this same canned SQL before we save the new one\n",
    "        conn = get_conn()\n",
    "        past_qas = []\n",
    "        if sql_item.get(\"name\"):\n",
    "            past_qas = get_past_answers_for_sql(conn, sql_item[\"name\"], limit=5)\n",
    "\n",
    "        cleaned_q = route.get(\"cleaned_question\", raw_question)\n",
    "        header = f\"**Question:** {cleaned_q}\\n\\n\"\n",
    "\n",
    "        medium_note = \"\"\n",
    "        if route[\"mode\"] == \"use_catalog_medium_conf\":\n",
    "            medium_note = (\n",
    "                \"Your question did not fully match a predefined SQL query. \"\n",
    "                \"We think this may be what you were asking.\\n\\n\"\n",
    "            )\n",
    "\n",
    "        assistant_text = (\n",
    "            header\n",
    "            + medium_note\n",
    "            + f\"Running canned SQL: `{sql_item['name']}`\"\n",
    "        )\n",
    "\n",
    "        # Base SQL result bubble\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(assistant_text)\n",
    "            if df is not None:\n",
    "                st.dataframe(df)\n",
    "            else:\n",
    "                st.info(\"The query returned no rows.\")\n",
    "\n",
    "        # Save table snapshot into history\n",
    "        if df is not None:\n",
    "            st.session_state.messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"type\": \"table\",\n",
    "                    \"content\": assistant_text,\n",
    "                    \"data\": df.head(20).to_dict(\"records\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            st.session_state.messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"type\": \"text\",\n",
    "                    \"content\": assistant_text + \"\\n\\n(No rows returned.)\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # --- 4a. LLM commentary on the SQL results ---------------------------\n",
    "        commentary = None\n",
    "        if df is not None:\n",
    "            commentary_prompt = build_sql_commentary_prompt(\n",
    "                user_question=raw_question,\n",
    "                sql_item=sql_item,\n",
    "                df=df,\n",
    "            )\n",
    "\n",
    "            commentary = llm_summarize(commentary_prompt)\n",
    "\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.markdown(\"**AI commentary on these results**\")\n",
    "                st.markdown(commentary)\n",
    "\n",
    "            st.session_state.messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"type\": \"text\",\n",
    "                    \"content\": \"AI commentary on these results:\\n\\n\" + commentary,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # --- 4b. Historical comparison using past Q&A -----------------------\n",
    "        hist_summary = None\n",
    "        if df is not None and past_qas:\n",
    "            hist_prompt = build_historical_comparison_prompt(\n",
    "                user_question=raw_question,\n",
    "                sql_item=sql_item,\n",
    "                current_df=df,\n",
    "                current_commentary=commentary,\n",
    "                past_qas=past_qas,\n",
    "            )\n",
    "\n",
    "            hist_summary = llm_summarize(hist_prompt)\n",
    "\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.markdown(\"**How this compares to previous answers**\")\n",
    "                st.markdown(hist_summary)\n",
    "\n",
    "            st.session_state.messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"type\": \"text\",\n",
    "                    \"content\": \"Comparison to history:\\n\\n\" + hist_summary,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # --- 4c. Save this Q&A into persistent history ----------------------\n",
    "        full_answer_text = assistant_text\n",
    "        if commentary:\n",
    "            full_answer_text += \"\\n\\nAI commentary on these results:\\n\\n\" + commentary\n",
    "        if hist_summary:\n",
    "            full_answer_text += \"\\n\\nComparison to history:\\n\\n\" + hist_summary\n",
    "\n",
    "        save_qa(conn, raw_question, full_answer_text, route[\"mode\"], sql_item)\n",
    "\n",
    "    # Fallback: pure LLM analysis\n",
    "    else:\n",
    "        bundle = build_light_bundle(user_text, chat_ctx, sql_ctx, cached_ans)\n",
    "        model_key = choose_model_key(use_full=False, needs_smart=False)\n",
    "\n",
    "        answer = llm_call(\n",
    "            model_key,\n",
    "            bundle,\n",
    "        )\n",
    "\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(answer)\n",
    "\n",
    "        st.session_state.messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"type\": \"text\",\n",
    "                \"content\": answer,\n",
    "            }\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
